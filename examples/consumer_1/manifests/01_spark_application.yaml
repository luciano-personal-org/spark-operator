# This script is used to run the spark-consumer application on EKS,
# users need to replace MY_BUCKET_NAME and MY_KAFKA_BROKERS_ADRESS to match your environment.
---
apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: spark-consumer
spec:
  type: Python
  pythonVersion: "3"
  #Policy: arn:aws:iam::992382765082:policy/FullS3SQSAccess
  #Role: arn:aws:iam::992382765082:role/EKS-S3-SQS-FullAccess
  # Buckets:
  # spark-luciano-test-1
  # spark-luciano-test-2
  # spark-luciano-test-3
  # spark-luciano-test-4
  # SQS:
  # https://sqs.us-east-1.amazonaws.com/992382765082/spark-luciano-test-1
  mode: cluster
  #image: "public.ecr.aws/data-on-eks/consumer-spark-streaming-3.3.2-kafka:1" # You can build your own image using the Dockerfile in this folder
  image: "992382765082.dkr.ecr.us-east-1.amazonaws.com/luciano/consumer:v6" # You can build your own image using the Dockerfile in this folder
  
  mainApplicationFile: "local:///app/app.py"
  sparkVersion: "3.3.2"
  deps:
    jars:
      - "local:///app/jars/commons-logging-1.1.3.jar"
      - "local:///app/jars/commons-pool2-2.11.1.jar"
      - "local:///app/jars/hadoop-client-api-3.3.2.jar"
      - "local:///app/jars/hadoop-client-runtime-3.3.2.jar"
      - "local:///app/jars/jsr305-3.0.0.jar"
      - "local:///app/jars/kafka-clients-2.8.1.jar"
      - "local:///app/jars/lz4-java-1.7.1.jar"
      - "local:///app/jars/scala-library-2.12.15.jar"
      - "local:///app/jars/slf4j-api-1.7.30.jar"
      - "local:///app/jars/snappy-java-1.1.8.1.jar"
      - "local:///app/jars/spark-sql-kafka-0-10_2.12-3.3.2.jar"
      - "local:///app/jars/spark-tags_2.12-3.3.2.jar"
      - "local:///app/jars/spark-token-provider-kafka-0-10_2.12-3.3.2.jar"
      - "local:///app/jars/iceberg-spark-runtime-3.3_2.12-1.0.0.jar"
      - "local:///app/jars/hadoop-aws-3.3.2.jar"
      - "local:///app/jars/aws-java-sdk-bundle-1.11.1026.jar"
      - "local:///app/jars/wildfly-openssl-1.0.7.Final.jar"
      - "local:///app/jars/parquet-avro-1.12.3.jar"
    # pyFiles:
    #   - "local:///app/dependencies/boto3.zip"
    # /opt/spark/bin/spark-submit --jars $(echo /app/jars/*.jar | tr ' ' ',') app.py
  sparkConf:
    "spark.app.name": "SQSToIceberg"
    "spark.jars.repositories": "https://repo1.maven.org/maven2/"
    "spark.sql.extensions": "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions"
    "spark.sql.catalog.local": "org.apache.iceberg.spark.SparkCatalog"
    "spark.sql.catalog.local.type": "hadoop"
    "spark.sql.catalog.local.warehouse": "s3a://spark-luciano-test-1/iceberg/warehouse/" # Replace bucket name with your S3 bucket name: s3_bucket_id_iceberg_bucket
    "spark.hadoop.fs.s3a.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem"
    "spark.hadoop.fs.s3a.aws.credentials.provider": "com.amazonaws.auth.DefaultAWSCredentialsProviderChain"
    "spark.sql.warehouse.dir": "s3a://spark-luciano-test-1/iceberg/warehouse/" # Replace bucket name with your S3 bucket name: s3_bucket_id_iceberg_bucket
    # "spark.metrics.conf.*.sink.prometheusServlet.class": "org.apache.spark.metrics.sink.PrometheusServlet"
    # "spark.metrics.conf.*.sink.prometheusServlet.path": "/metrics"
    # "spark.metrics.conf.master.sink.prometheusServlet.path": "/metrics/master"
    # "spark.metrics.conf.applications.sink.prometheusServlet.path": "/metrics/applications"
    # "spark.ui.prometheus.enabled": "true"
    # "spark.ui.prometheus.port": "4040"
  restartPolicy:
    type: OnFailure
    onFailureRetries: 2
    onFailureRetryInterval: 10
    onSubmissionFailureRetries: 3
    onSubmissionFailureRetryInterval: 20
  dynamicAllocation:
    enabled: true
    initialExecutors: 5
    minExecutors: 5
    maxExecutors: 10
  driver:
    cores: 1
    coreLimit: "1200m"
    memory: "1024m"
    labels:
      version: "3.3.2"
      app: spark
    serviceAccount: spark-sa-test
    # sparkConf:
    #   "spark.submit.pyFiles": "local:///opt/spark/python/lib/boto3.zip"
    env:
      - name: S3_BUCKET_NAME
        value: "spark-luciano-test-1" # Replace with your S3 bucket name: s3_bucket_id_iceberg_bucket
      - name: KAFKA_ADDRESS
        value: "b-1.kafkademospark.d0c8nh.c13.kafka.us-east-1.amazonaws.com:9092,b-2.kafkademospark.d0c8nh.c13.kafka.us-east-1.amazonaws.com:9092" # Replace with your Kafka brokers address: bootstrap_brokers
        # value: "b-1.kafkademospark.mkjcj4.c12.kafka.us-west-2.amazonaws.com:9092,b-2.kafkademospark.mkjcj4.c12.kafka.us-west-2.amazonaws.com:9092"
      # - name: PYTHONPATH
      #   value: $PYTHONPATH:$SPARK_HOME/python/lib/py4j-0.10.9.5-src.zip:$SPARK_HOME/python/lib/pyspark.zip:$SPARK_HOME/python/lib/boto3.zip
  executor:
    cores: 2
    memory: "8g"
    labels:
      version: "3.3.2"
      app: spark
    serviceAccount: spark-sa-test
    # sparkConf:
    #   "spark.submit.pyFiles": "local:///opt/spark/python/lib/boto3.zip"
    env:
      - name: S3_BUCKET_NAME
        value: "spark-luciano-test-1" # Replace with your S3 bucket name: s3_bucket_id_iceberg_bucket
      - name: KAFKA_ADDRESS
        value: "b-1.kafkademospark.d0c8nh.c13.kafka.us-east-1.amazonaws.com:9092,b-2.kafkademospark.d0c8nh.c13.kafka.us-east-1.amazonaws.com:9092" # Replace with your Kafka brokers address: bootstrap_brokers
        # value: "b-1.kafkademospark.mkjcj4.c12.kafka.us-west-2.amazonaws.com:9092,b-2.kafkademospark.mkjcj4.c12.kafka.us-west-2.amazonaws.com:9092"
      # - name: PYTHONPATH
      #   value: $PYTHONPATH:$SPARK_HOME/python/lib/py4j-0.10.9.5-src.zip:$SPARK_HOME/python/lib/pyspark.zip:$SPARK_HOME/python/lib/boto3.zip
